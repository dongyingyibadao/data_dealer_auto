"""
ä½¿ç”¨Qwen/Deepseek LLMç”Ÿæˆä»»åŠ¡æè¿°
"""
import json
from typing import List, Dict, Optional
import requests
from abc import ABC, abstractmethod
import base64
import io
try:
    from PIL import Image
except ImportError:
    import sys
    print("Installing Pillow...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "Pillow"])
    from PIL import Image
import numpy as np
import torch


class LLMProvider(ABC):
    """LLMæä¾›è€…åŸºç±»"""
    
    @abstractmethod
    def generate_task_description(self, 
                                 action_type: str,
                                 original_task: str,
                                 context: Dict) -> str:
        """
        ç”Ÿæˆä»»åŠ¡æè¿°
        
        Args:
            action_type: 'pick' æˆ– 'place'
            original_task: åŸå§‹ä»»åŠ¡æè¿°
            context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯ (åŒ…å«å›¾åƒç­‰)
            
        Returns:
            ç”Ÿæˆçš„ä»»åŠ¡æè¿°
        """
        pass


class GPTVLM(LLMProvider):
    """
    ä½¿ç”¨GPT-4o (VLM) ç”Ÿæˆä»»åŠ¡æè¿°
    """
    
    def __init__(self, api_key: str = None, api_base: str = None, api_version: str = None, model: str = "gpt-4o", fast_mode: bool = False):
        self.api_key = api_key
        self.api_base = api_base
        self.api_version = api_version
        self.model = model
        self.fast_mode = fast_mode  # å¿«é€Ÿæ¨¡å¼ï¼šä»…ä½¿ç”¨2å¸§ï¼ˆcam1é¦–å°¾å¸§ï¼‰
        self.available = api_key is not None
        
    def _encode_image(self, image_data):
        """å°†å›¾åƒè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
        # 1. å¤„ç† Tensor -> Numpy
        if hasattr(image_data, 'cpu'):
            image_data = image_data.cpu()
        if hasattr(image_data, 'numpy'):
            image_data = image_data.numpy()
            
        # 2. å¤„ç† Numpy æ•°ç»„
        if isinstance(image_data, np.ndarray):
            # ç¡®ä¿æ˜¯ HWC æ ¼å¼
            # å‡è®¾: å¦‚æœ shape[0] æ˜¯ 3ï¼Œä¸”åé¢ä¸¤ä¸ªç»´åº¦æ¯” 3 å¤§ï¼Œåˆ™æ˜¯ CHW
            if image_data.ndim == 3 and image_data.shape[0] == 3 and image_data.shape[1] > 3 and image_data.shape[2] > 3:
                image_data = image_data.transpose(1, 2, 0)
            
            # ç¡®ä¿å€¼åœ¨ 0-255 ä¹‹é—´ä¸”ä¸º uint8
            if image_data.dtype != np.uint8:
                if image_data.max() <= 1.0:
                    image_data = (image_data * 255).astype(np.uint8)
                else:
                    image_data = image_data.astype(np.uint8)
            
            img = Image.fromarray(image_data)
            
        elif isinstance(image_data, Image.Image):
            img = image_data
        else:
            # å°è¯•ä½œä¸º PIL Image æ‰“å¼€ (å¦‚æœæ˜¯è·¯å¾„å­—ç¬¦ä¸²)
            try:
                img = Image.open(image_data)
            except:
                raise ValueError(f"Unsupported image type: {type(image_data)}")
            
        buffered = io.BytesIO()
        img.save(buffered, format="JPEG")
        return base64.b64encode(buffered.getvalue()).decode('utf-8')

    def generate_task_description(self, 
                                 action_type: str,
                                 original_task: str,
                                 context: Dict = None) -> str:
        if not self.available:
            print("âš ï¸  GPT API Keyæœªæä¾›ï¼Œæ— æ³•ä½¿ç”¨VLM")
            return f"{action_type} object"
            
        try:
            import openai
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Azure OpenAI
            if self.api_version:
                from openai import AzureOpenAI
                client = AzureOpenAI(
                    api_key=self.api_key,
                    azure_endpoint=self.api_base,
                    api_version=self.api_version
                )
            else:
                client = openai.OpenAI(
                    api_key=self.api_key,
                    base_url=self.api_base
                )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¤ä¸ªæ‘„åƒå¤´çš„å›¾åƒ
            first_cam1 = context.get('first_frame_cam1')
            last_cam1 = context.get('last_frame_cam1')
            key_cam1 = context.get('key_frame_cam1')
            first_cam2 = context.get('first_frame_cam2')
            last_cam2 = context.get('last_frame_cam2')
            key_cam2 = context.get('key_frame_cam2')
            
            # å¿«é€Ÿæ¨¡å¼ï¼šåªéœ€è¦cam1çš„é¦–å°¾å¸§
            if self.fast_mode:
                if first_cam1 is None or last_cam1 is None:
                    print("âš ï¸  GPT VLM ç¼ºå°‘å›¾åƒæ•°æ®")
                    return f"{action_type} object"
                
                # ä»…ç¼–ç cam1çš„é¦–å°¾ä¸¤å¸§
                first_cam1_b64 = self._encode_image(first_cam1)
                last_cam1_b64 = self._encode_image(last_cam1)
                
                # æ„å»ºå›¾åƒè¯´æ˜ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
                cam_info = "æˆ‘æä¾›äº†æ‘„åƒå¤´çš„å›¾åƒ"
                img_order = """
å›¾åƒé¡ºåºï¼š
1. é¦–å¸§ï¼ˆåŠ¨ä½œå¼€å§‹å‰ï¼‰
2. å°¾å¸§ï¼ˆåŠ¨ä½œå®Œæˆåï¼‰"""
            else:
                # ç²¾ç»†æ¨¡å¼ï¼šä½¿ç”¨æ‰€æœ‰å¸§
                if first_cam1 is None or last_cam1 is None or key_cam1 is None:
                    print("âš ï¸  GPT VLM ç¼ºå°‘å›¾åƒæ•°æ®")
                    return f"{action_type} object"
                
                # ç¼–ç æ‰€æœ‰å›¾åƒ
                first_cam1_b64 = self._encode_image(first_cam1)
                last_cam1_b64 = self._encode_image(last_cam1)
                key_cam1_b64 = self._encode_image(key_cam1)
                
                # å¦‚æœæœ‰ç¬¬äºŒä¸ªæ‘„åƒå¤´çš„å›¾åƒï¼Œä¹Ÿç¼–ç 
                has_cam2 = first_cam2 is not None and last_cam2 is not None and key_cam2 is not None
                if has_cam2:
                    first_cam2_b64 = self._encode_image(first_cam2)
                    last_cam2_b64 = self._encode_image(last_cam2)
                    key_cam2_b64 = self._encode_image(key_cam2)
                else:
                    has_cam2 = False
                
                # æ„å»ºå›¾åƒè¯´æ˜ï¼ˆç²¾ç»†æ¨¡å¼ï¼‰
                cam_info = "æˆ‘æä¾›äº†æ¥è‡ªä¸¤ä¸ªä¸åŒè§†è§’æ‘„åƒå¤´çš„å›¾åƒ" if has_cam2 else "æˆ‘æä¾›äº†æ‘„åƒå¤´çš„å›¾åƒ"
                img_order = """
å›¾åƒé¡ºåºï¼š
1-3. Camera 1 (æ•´ä½“åœºæ™¯è§†è§’): é¦–å¸§ã€å…³é”®å¸§(åŠ¨ä½œå‘ç”Ÿæ—¶åˆ»)ã€å°¾å¸§
4-6. Camera 2 (æ“ä½œç»†èŠ‚è§†è§’): é¦–å¸§ã€å…³é”®å¸§(åŠ¨ä½œå‘ç”Ÿæ—¶åˆ»)ã€å°¾å¸§""" if has_cam2 else """
å›¾åƒé¡ºåºï¼š
1. é¦–å¸§
2. å…³é”®å¸§(åŠ¨ä½œå‘ç”Ÿæ—¶åˆ»)
3. å°¾å¸§"""
            
            # æ ¹æ®æ¨¡å¼æ„å»ºä¸åŒçš„prompt
            if self.fast_mode:
                prompt = f"""
åŸå§‹ä»»åŠ¡æè¿°: "{original_task}"
åŠ¨ä½œç±»å‹: "{action_type}" (pick=æŠ“å–ç‰©ä½“, place=æ”¾ç½®ç‰©ä½“)

{cam_info}ï¼Œå¸®åŠ©ä½ ç†è§£è¿™ä¸ªåŠ¨ä½œç‰‡æ®µã€‚{img_order}

é‡è¦è¯´æ˜ï¼š
1. **æ¯ä¸ªåŠ¨ä½œç‰‡æ®µåªæ“ä½œä¸€ä¸ªç‰©ä½“** - æœºæ¢°è‡‚æ¯æ¬¡åªèƒ½æŠ“å–æˆ–æ”¾ç½®ä¸€ä¸ªç‰©ä½“
2. **å¯¹æ¯”é¦–å°¾å¸§çš„å˜åŒ–** - æ³¨æ„å“ªä¸ªç‰©ä½“çš„ä½ç½®å‘ç”Ÿäº†æ”¹å˜
3. **æ³¨æ„ç‰©ä½“æè¿°çš„å®Œæ•´æ€§** - ä¾‹å¦‚"yellow and white mug"æ˜¯ä¸€ä¸ªé»„ç™½ç›¸é—´çš„æ¯å­ï¼Œä¸æ˜¯ä¸¤ä¸ªæ¯å­

è§‚å¯Ÿè¦ç‚¹ï¼š
- å¯¹æ¯”é¦–å°¾ä¸¤å¸§ï¼Œå“ªä¸ªç‰©ä½“çš„ä½ç½®å‘ç”Ÿäº†å˜åŒ–ï¼Ÿ
- è¯¥ç‰©ä½“çš„å®Œæ•´æè¿°æ˜¯ä»€ä¹ˆï¼Ÿ(åŒ…æ‹¬é¢œè‰²ã€å½¢çŠ¶ç­‰ç‰¹å¾)

è¾“å‡ºæ ¼å¼ï¼š
- æ ¼å¼å¿…é¡»æ˜¯: "pick [object]" æˆ– "place [object] [location]"
- [object]å¿…é¡»æ˜¯å®Œæ•´çš„ç‰©ä½“æè¿°(å¦‚: "white mug", "yellow and white mug")
- åªè¿”å›ä¸€è¡Œæè¿°ï¼Œä¸è¦å…¶ä»–å†…å®¹

ç¤ºä¾‹ï¼š
åŸå§‹ä»»åŠ¡: "put the yellow and white mug on the plate"
åŠ¨ä½œç±»å‹: "pick"
æ­£ç¡®è¾“å‡º: pick the yellow and white mug
"""
            else:
                prompt = f"""
åŸå§‹ä»»åŠ¡æè¿°: "{original_task}"
åŠ¨ä½œç±»å‹: "{action_type}" (pick=æŠ“å–ç‰©ä½“, place=æ”¾ç½®ç‰©ä½“)

{cam_info}ï¼Œå¸®åŠ©ä½ ç†è§£è¿™ä¸ªåŠ¨ä½œç‰‡æ®µã€‚{img_order}

é‡è¦è¯´æ˜ï¼š
1. **æ¯ä¸ªåŠ¨ä½œç‰‡æ®µåªæ“ä½œä¸€ä¸ªç‰©ä½“** - æœºæ¢°è‡‚æ¯æ¬¡åªèƒ½æŠ“å–æˆ–æ”¾ç½®ä¸€ä¸ªç‰©ä½“
2. **Camera 1æä¾›æ•´ä½“åœºæ™¯**ï¼ŒCamera 2æä¾›æ“ä½œç»†èŠ‚å’Œè¿‘è·ç¦»è§†è§’
3. **ä»”ç»†è¯†åˆ«ç‰©ä½“ç‰¹å¾** - æ³¨æ„ç‰©ä½“çš„é¢œè‰²ã€å½¢çŠ¶ã€çº¹ç†ç­‰ç‰¹å¾
4. **æ³¨æ„ç‰©ä½“æè¿°çš„å®Œæ•´æ€§** - ä¾‹å¦‚"yellow and white mug"æ˜¯ä¸€ä¸ªé»„ç™½ç›¸é—´çš„æ¯å­ï¼Œä¸æ˜¯ä¸¤ä¸ªæ¯å­

è§‚å¯Ÿè¦ç‚¹ï¼š
- å¯¹æ¯”å…³é”®å¸§å‰åï¼Œå“ªä¸ªç‰©ä½“çš„ä½ç½®å‘ç”Ÿäº†å˜åŒ–ï¼Ÿ
- æœºæ¢°è‡‚å¤¹çˆªæ¥è§¦æˆ–æ“ä½œçš„æ˜¯å“ªä¸ªå…·ä½“ç‰©ä½“ï¼Ÿ
- è¯¥ç‰©ä½“çš„å®Œæ•´æè¿°æ˜¯ä»€ä¹ˆï¼Ÿ(åŒ…æ‹¬é¢œè‰²ã€å›¾æ¡ˆç­‰ç‰¹å¾)

è¾“å‡ºæ ¼å¼ï¼š
- æ ¼å¼å¿…é¡»æ˜¯: "pick [object]" æˆ– "place [object] [location]"
- [object]å¿…é¡»æ˜¯å®Œæ•´çš„ç‰©ä½“æè¿°(å¦‚: "white mug", "yellow and white mug", "chocolate pudding")
- åªè¿”å›ä¸€è¡Œæè¿°ï¼Œä¸è¦å…¶ä»–å†…å®¹

ç¤ºä¾‹ï¼š
åŸå§‹ä»»åŠ¡: "put the yellow and white mug on the plate"
åŠ¨ä½œç±»å‹: "pick"
æ­£ç¡®è¾“å‡º: pick the yellow and white mug

åŸå§‹ä»»åŠ¡: "put the red bowl and the blue cup on the table"  
åŠ¨ä½œç±»å‹: "place"
(è§‚å¯Ÿå›¾åƒåå‘ç°æ“ä½œçš„æ˜¯è“è‰²æ¯å­)
æ­£ç¡®è¾“å‡º: place the blue cup on the table
"""
            
            # æ„å»ºå›¾åƒå†…å®¹åˆ—è¡¨
            image_contents = [{"type": "text", "text": prompt}]
            
            if self.fast_mode:
                # å¿«é€Ÿæ¨¡å¼ï¼šä»…ä¸Šä¼ cam1çš„é¦–å°¾ä¸¤å¸§
                image_contents.extend([
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{first_cam1_b64}"}},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{last_cam1_b64}"}},
                ])
            else:
                # ç²¾ç»†æ¨¡å¼ï¼šä¸Šä¼ cam1çš„ä¸‰å¸§
                image_contents.extend([
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{first_cam1_b64}"}},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{key_cam1_b64}"}},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{last_cam1_b64}"}},
                ])
                
                # å¦‚æœæœ‰ç¬¬äºŒä¸ªæ‘„åƒå¤´çš„å›¾åƒï¼Œæ·»åŠ 
                if has_cam2:
                    image_contents.extend([
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{first_cam2_b64}"}},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{key_cam2_b64}"}},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{last_cam2_b64}"}},
                    ])

            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": image_contents}],
                max_tokens=50
            )
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å“åº”
            if response.choices and len(response.choices) > 0:
                message = response.choices[0].message
                if message.content is None:
                    print(f"âš ï¸  GPT è¿”å›äº†ç©ºå†…å®¹")
                    print(f"   Response: {response}")
                    print(f"   Message: {message}")
                    return f"{action_type} object"
                return message.content.strip()
            else:
                print(f"âš ï¸  GPT è¿”å›äº†ç©ºçš„ choices")
                print(f"   Response: {response}")
                return f"{action_type} object"
            
        except Exception as e:
            print(f"âš ï¸  GPT VLM è°ƒç”¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"{action_type} object"


class QwenLLM(LLMProvider):
    """
    ä½¿ç”¨é˜¿é‡ŒQwenæ¨¡å‹ï¼ˆé€šè¿‡å…¼å®¹OpenAIçš„APIï¼‰
    """
    
    def __init__(self, api_key: str = None, api_base: str = None, model: str = "qwen-turbo"):
        """
        åˆå§‹åŒ–Qwen LLM
        
        Args:
            api_key: APIå¯†é’¥
            api_base: APIåŸºç¡€URL
            model: æ¨¡å‹åç§°
        """
        self.api_key = api_key
        self.api_base = api_base or "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.model = model
        self.available = api_key is not None
    
    def generate_task_description(self, 
                                 action_type: str,
                                 original_task: str,
                                 context: Dict = None) -> str:
        """
        ä½¿ç”¨Qwenç”Ÿæˆä»»åŠ¡æè¿°
        """
        if not self.available:
            return self._generate_local(action_type, original_task, context)
        
        try:
            import openai
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.api_base
            )
            
            prompt = self._build_prompt(action_type, original_task, context)
            
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººä»»åŠ¡æè¿°ç”Ÿæˆå™¨ã€‚æ ¹æ®åŸå§‹ä»»åŠ¡å’Œæ“ä½œç±»å‹ï¼Œç”Ÿæˆç®€æ´çš„ä»»åŠ¡æè¿°ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=100
            )
            
            return response.choices[0].message.content.strip()
        
        except Exception as e:
            print(f"âš ï¸  Qwen APIè°ƒç”¨å¤±è´¥: {e}ï¼Œä½¿ç”¨æœ¬åœ°æ–¹æ³•ç”Ÿæˆ")
            return self._generate_local(action_type, original_task, context)
    
    @staticmethod
    def _build_prompt(action_type: str, original_task: str, context: Dict = None) -> str:
        """æ„å»ºæç¤ºè¯"""
        return f"""
æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´çš„æœºå™¨äººä»»åŠ¡æè¿°ï¼š

åŸå§‹ä»»åŠ¡: {original_task}
æ“ä½œç±»å‹: {'å¤¹çˆªå…³é—­ï¼ˆæŠ“å–ï¼‰' if action_type == 'pick' else 'å¤¹çˆªæ‰“å¼€ï¼ˆæ”¾ç½®ï¼‰'}

è¦æ±‚ï¼š
1. ä¿ç•™åŸå§‹ä»»åŠ¡ä¸­å…³é”®çš„ç‰©ä½“å’Œä½ç½®ä¿¡æ¯
2. æ ¹æ®æ“ä½œç±»å‹ï¼ˆpick/placeï¼‰ç”Ÿæˆå¯¹åº”çš„åŠ¨è¯
3. Pickæ“ä½œä½¿ç”¨"pick up"æˆ–"grab"ï¼ŒPlaceæ“ä½œä½¿ç”¨"put"æˆ–"place"
4. ç”Ÿæˆçš„æè¿°åº”è¯¥ç®€æ´ï¼Œä¸è¶…è¿‡20ä¸ªå•è¯
5. ä»…è¿”å›ç”Ÿæˆçš„æè¿°ï¼Œä¸éœ€è¦å…¶ä»–å†…å®¹

ç”Ÿæˆçš„ä»»åŠ¡æè¿°ï¼š
"""
    
    @staticmethod
    def _generate_local(action_type: str, original_task: str, context: Dict = None) -> str:
        """æœ¬åœ°ç”Ÿæˆä»»åŠ¡æè¿°ï¼ˆæ— APIæ—¶ä½¿ç”¨ï¼‰"""
        # æå–ç‰©ä½“å’Œä½ç½®ä¿¡æ¯
        words = original_task.lower().split()
        
        # æ‰¾åˆ°å…³é”®è¯
        object_words = []
        location_words = []
        
        # ç®€å•çš„å…³é”®è¯æå–
        prepositions = ['the', 'on', 'in', 'at', 'to', 'from', 'under', 'above', 'next', 'between']
        for i, word in enumerate(words):
            if word not in prepositions and word not in ['and', 'or', 'put', 'pick', 'up', 'open', 'close']:
                if i < len(words) - 1 and words[i + 1] not in prepositions:
                    object_words.append(word)
                elif i == len(words) - 1:
                    object_words.append(word)
        
        # ç”Ÿæˆæè¿°
        if action_type == 'pick':
            verb = 'pick up'
            if object_words:
                obj = object_words[0]
                return f"{verb} the {obj}"
            else:
                return f"{verb} the object"
        else:  # place
            verb = 'put'
            if len(object_words) >= 2:
                obj = object_words[0]
                loc = object_words[1]
                return f"{verb} the {obj} on the {loc}"
            elif len(object_words) >= 1:
                obj = object_words[0]
                return f"{verb} the {obj}"
            else:
                return f"{verb} the object"


class DeepseekLLM(LLMProvider):
    """
    ä½¿ç”¨Deepseekæ¨¡å‹ï¼ˆé€šè¿‡OpenAIå…¼å®¹APIï¼‰
    """
    
    def __init__(self, api_key: str = None, api_base: str = None, model: str = "deepseek-chat"):
        """
        åˆå§‹åŒ–Deepseek LLM
        
        Args:
            api_key: APIå¯†é’¥
            api_base: APIåŸºç¡€URL
            model: æ¨¡å‹åç§°
        """
        self.api_key = api_key
        self.api_base = api_base or "https://api.deepseek.com/beta"
        self.model = model
        self.available = api_key is not None
    
    def generate_task_description(self, 
                                 action_type: str,
                                 original_task: str,
                                 context: Dict = None) -> str:
        """
        ä½¿ç”¨Deepseekç”Ÿæˆä»»åŠ¡æè¿°
        """
        if not self.available:
            return self._generate_local(action_type, original_task, context)
        
        try:
            import openai
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.api_base
            )
            
            prompt = self._build_prompt(action_type, original_task, context)
            
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººä»»åŠ¡æè¿°ç”Ÿæˆå™¨ã€‚æ ¹æ®åŸå§‹ä»»åŠ¡å’Œæ“ä½œç±»å‹ï¼Œç”Ÿæˆç®€æ´çš„ä»»åŠ¡æè¿°ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=100
            )
            
            return response.choices[0].message.content.strip()
        
        except Exception as e:
            print(f"âš ï¸  Deepseek APIè°ƒç”¨å¤±è´¥: {e}ï¼Œä½¿ç”¨æœ¬åœ°æ–¹æ³•ç”Ÿæˆ")
            return self._generate_local(action_type, original_task, context)
    
    @staticmethod
    def _build_prompt(action_type: str, original_task: str, context: Dict = None) -> str:
        """æ„å»ºæç¤ºè¯"""
        return f"""
æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´çš„æœºå™¨äººä»»åŠ¡æè¿°ï¼š

åŸå§‹ä»»åŠ¡: {original_task}
æ“ä½œç±»å‹: {'å¤¹çˆªå…³é—­ï¼ˆæŠ“å–ï¼‰' if action_type == 'pick' else 'å¤¹çˆªæ‰“å¼€ï¼ˆæ”¾ç½®ï¼‰'}

è¦æ±‚ï¼š
1. ä¿ç•™åŸå§‹ä»»åŠ¡ä¸­å…³é”®çš„ç‰©ä½“å’Œä½ç½®ä¿¡æ¯
2. æ ¹æ®æ“ä½œç±»å‹ï¼ˆpick/placeï¼‰ç”Ÿæˆå¯¹åº”çš„åŠ¨è¯
3. Pickæ“ä½œä½¿ç”¨"pick up"æˆ–"grab"ï¼ŒPlaceæ“ä½œä½¿ç”¨"put"æˆ–"place"
4. ç”Ÿæˆçš„æè¿°åº”è¯¥ç®€æ´ï¼Œä¸è¶…è¿‡20ä¸ªå•è¯
5. ä»…è¿”å›ç”Ÿæˆçš„æè¿°ï¼Œä¸éœ€è¦å…¶ä»–å†…å®¹

ç”Ÿæˆçš„ä»»åŠ¡æè¿°ï¼š
"""
    
    @staticmethod
    def _generate_local(action_type: str, original_task: str, context: Dict = None) -> str:
        """æœ¬åœ°ç”Ÿæˆä»»åŠ¡æè¿°ï¼ˆæ— APIæ—¶ä½¿ç”¨ï¼‰"""
        # æå–ç‰©ä½“å’Œä½ç½®ä¿¡æ¯
        words = original_task.lower().split()
        
        # æ‰¾åˆ°å…³é”®è¯
        object_words = []
        prepositions = ['the', 'on', 'in', 'at', 'to', 'from', 'under', 'above', 'next', 'between']
        
        for i, word in enumerate(words):
            if word not in prepositions and word not in ['and', 'or', 'put', 'pick', 'up', 'open', 'close']:
                if i < len(words) - 1 and words[i + 1] not in prepositions:
                    object_words.append(word)
                elif i == len(words) - 1:
                    object_words.append(word)
        
        # ç”Ÿæˆæè¿°
        if action_type == 'pick':
            verb = 'pick up'
            if object_words:
                obj = object_words[0]
                return f"{verb} the {obj}"
            else:
                return f"{verb} the object"
        else:  # place
            verb = 'put'
            if len(object_words) >= 2:
                obj = object_words[0]
                loc = object_words[1]
                return f"{verb} the {obj} on the {loc}"
            elif len(object_words) >= 1:
                obj = object_words[0]
                return f"{verb} the {obj}"
            else:
                return f"{verb} the object"


class TaskDescriptionGenerator:
    """
    ä»»åŠ¡æè¿°ç”Ÿæˆå™¨
    """
    
    def __init__(self, provider: str = "local", **kwargs):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            provider: 'qwen', 'deepseek', 'gpt', æˆ– 'local'
            **kwargs: ä¼ é€’ç»™LLMæä¾›è€…çš„å‚æ•°ï¼ˆåŒ…æ‹¬fast_modeï¼‰
        """
        # è¿‡æ»¤æ‰ None å€¼çš„å‚æ•°ï¼Œé¿å…ä¼ é€’ç»™æ„é€ å‡½æ•°
        kwargs = {k: v for k, v in kwargs.items() if v is not None}
        
        if provider.lower() == 'qwen':
            self.llm = QwenLLM(**kwargs)
        elif provider.lower() == 'deepseek':
            self.llm = DeepseekLLM(**kwargs)
        elif provider.lower() == 'gpt':
            self.llm = GPTVLM(**kwargs)
        else:
            self.llm = QwenLLM()  # é»˜è®¤ä½¿ç”¨æœ¬åœ°æ–¹æ³•
    
    def generate_descriptions(self, 
                            frame_ranges: List[Dict],
                            dataset = None,
                            cache: Dict = None) -> List[Dict]:
        """
        ä¸ºæ‰€æœ‰å¸§èŒƒå›´ç”Ÿæˆä»»åŠ¡æè¿°
        
        Args:
            frame_ranges: å¸§èŒƒå›´åˆ—è¡¨
            dataset: LeRobotæ•°æ®é›† (ç”¨äºVLMè·å–å›¾åƒ)
            cache: ç¼“å­˜å·²ç”Ÿæˆçš„æè¿°
            
        Returns:
            æ·»åŠ äº†new_taskå­—æ®µçš„å¸§èŒƒå›´åˆ—è¡¨
        """
        if cache is None:
            cache = {}
        
        result = []
        
        print(f"ğŸ¤– ä½¿ç”¨{self.llm.__class__.__name__}ç”Ÿæˆä»»åŠ¡æè¿°...")
        
        for i, frame_range in enumerate(frame_ranges):
            if i % 10 == 0:
                print(f"  è¿›åº¦: {i}/{len(frame_ranges)}")
            
            # åˆ›å»ºç¼“å­˜é”®
            # æ³¨æ„ï¼šå¯¹äºVLMï¼Œå¦‚æœåªç”¨action_typeå’Œtaskåškeyï¼Œä¼šå¿½ç•¥å›¾åƒå·®å¼‚ã€‚
            # å¦‚æœæ˜¯VLMï¼Œæˆ‘ä»¬å¯èƒ½ä¸åº”è¯¥ä½¿ç”¨ç®€å•çš„ç¼“å­˜ï¼Œæˆ–è€…åº”è¯¥åŒ…å«keyframe_index
            if isinstance(self.llm, GPTVLM):
                cache_key = f"{frame_range['action_type']}_{frame_range['task']}_{frame_range['keyframe_index']}"
            else:
                cache_key = f"{frame_range['action_type']}_{frame_range['task']}"
            
            if cache_key in cache:
                new_task = cache[cache_key]
            else:
                # å‡†å¤‡ä¸Šä¸‹æ–‡
                context = {'episode_index': frame_range['episode_index']}
                
                # å¦‚æœæ˜¯VLMä¸”æä¾›äº†æ•°æ®é›†ï¼Œè·å–å›¾åƒ
                if isinstance(self.llm, GPTVLM) and dataset is not None:
                    try:
                        start_idx = int(frame_range['frame_start'])
                        end_idx = int(frame_range['frame_end']) - 1 # frame_end is exclusive
                        key_idx = int(frame_range['keyframe_index'])
                        
                        # è·å–é¦–å°¾å¸§å’Œå…³é”®å¸§ (ä¸¤ä¸ªæ‘„åƒå¤´)
                        # LeRobot dataset returns dict with 'observation.images.image' and 'observation.images.image2'
                        first_item = dataset[start_idx]
                        last_item = dataset[end_idx]
                        key_item = dataset[key_idx]
                        
                        # Cam1 å›¾åƒ
                        context['first_frame_cam1'] = first_item['observation.images.image']
                        context['last_frame_cam1'] = last_item['observation.images.image']
                        context['key_frame_cam1'] = key_item['observation.images.image']
                        
                        # Cam2 å›¾åƒ
                        context['first_frame_cam2'] = first_item['observation.images.image2']
                        context['last_frame_cam2'] = last_item['observation.images.image2']
                        context['key_frame_cam2'] = key_item['observation.images.image2']
                    except Exception as e:
                        print(f"âš ï¸  è·å–å›¾åƒå¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                
                # ç”Ÿæˆæ–°çš„ä»»åŠ¡æè¿°
                new_task = self.llm.generate_task_description(
                    action_type=frame_range['action_type'],
                    original_task=frame_range['task'],
                    context=context
                )
                cache[cache_key] = new_task
            
            # æ·»åŠ åˆ°ç»“æœ
            range_with_desc = frame_range.copy()
            range_with_desc['new_task'] = new_task
            result.append(range_with_desc)
        
        print(f"âœ“ ä»»åŠ¡æè¿°ç”Ÿæˆå®Œæˆ")
        return result


if __name__ == '__main__':
    # æµ‹è¯•æœ¬åœ°ç”Ÿæˆ
    generator = TaskDescriptionGenerator(provider='local')
    
    test_ranges = [
        {
            'action_type': 'pick',
            'task': 'put both moka pots on the stove',
            'episode_index': 376
        },
        {
            'action_type': 'place',
            'task': 'put both moka pots on the stove',
            'episode_index': 376
        }
    ]
    
    results = generator.generate_descriptions(test_ranges)
    for r in results:
        print(f"{r['action_type']}: {r['task']} -> {r['new_task']}")

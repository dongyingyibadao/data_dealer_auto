#!/usr/bin/env python3
"""
å†…å­˜è¯Šæ–­è„šæœ¬ - ä¼°ç®—æ‰€éœ€çš„ batch_size
"""

import psutil
import sys


def get_available_memory_gb():
    """è·å–å¯ç”¨å†…å­˜ï¼ˆGBï¼‰"""
    mem = psutil.virtual_memory()
    return mem.available / (1024 ** 3)


def estimate_batch_size(total_frames, avg_frames_per_episode=60):
    """
    ä¼°ç®—åˆé€‚çš„batch_size
    
    Args:
        total_frames: æ•°æ®é›†æ€»å¸§æ•°
        avg_frames_per_episode: å¹³å‡æ¯ä¸ªepisodeçš„å¸§æ•°
    
    Returns:
        æ¨èçš„batch_size
    """
    available_memory = get_available_memory_gb()
    
    # æ¯å¸§å¤§çº¦å ç”¨1.5MBï¼ˆåŒ…å«2ä¸ªæ‘„åƒå¤´å›¾åƒï¼‰
    memory_per_frame_mb = 1.5
    
    # ä¿ç•™40%çš„å†…å­˜ç”¨äºç³»ç»Ÿå’Œå…¶ä»–å¼€é”€
    usable_memory_gb = available_memory * 0.6
    
    # è®¡ç®—å¯ä»¥åŒæ—¶å¤„ç†çš„å¸§æ•°
    max_frames_in_memory = (usable_memory_gb * 1024) / memory_per_frame_mb
    
    # è®¡ç®—batch_size
    batch_size = int(max_frames_in_memory / avg_frames_per_episode)
    
    return max(10, min(batch_size, 300))  # é™åˆ¶åœ¨10-300ä¹‹é—´


def print_recommendations(total_frames):
    """æ‰“å°æ¨èé…ç½®"""
    available_memory = get_available_memory_gb()
    total_memory = psutil.virtual_memory().total / (1024 ** 3)
    used_memory = psutil.virtual_memory().used / (1024 ** 3)
    
    print("=" * 60)
    print("ğŸ” ç³»ç»Ÿå†…å­˜åˆ†æ")
    print("=" * 60)
    print(f"æ€»å†…å­˜: {total_memory:.2f} GB")
    print(f"å·²ä½¿ç”¨: {used_memory:.2f} GB ({(used_memory/total_memory)*100:.1f}%)")
    print(f"å¯ç”¨å†…å­˜: {available_memory:.2f} GB")
    print()
    
    # ä¼°ç®—ä¸åŒåœºæ™¯ä¸‹çš„batch_size
    scenarios = [
        ("å°å‹episode (30å¸§)", 30),
        ("ä¸­å‹episode (60å¸§)", 60),
        ("å¤§å‹episode (100å¸§)", 100),
    ]
    
    print("ğŸ“Š æ¨èé…ç½®:")
    print("-" * 60)
    
    for scenario_name, avg_frames in scenarios:
        batch_size = estimate_batch_size(total_frames, avg_frames)
        memory_usage = (batch_size * avg_frames * 1.5) / 1024  # GB
        
        print(f"\n{scenario_name}:")
        print(f"  æ¨è batch_size: {batch_size}")
        print(f"  é¢„è®¡å†…å­˜å ç”¨: ~{memory_usage:.2f} GB")
        print(f"  å‘½ä»¤ç¤ºä¾‹:")
        print(f"    python auto_cut_dataset.py --batch-size {batch_size} ...")
    
    print()
    print("=" * 60)
    print("âš ï¸  æ³¨æ„äº‹é¡¹:")
    print("=" * 60)
    print("1. å¦‚æœå†…å­˜ä¸è¶³ï¼Œå‡å° batch_size")
    print("2. å¦‚æœå†…å­˜å……è¶³ï¼Œå¯ä»¥é€‚å½“å¢åŠ  batch_size æå‡é€Ÿåº¦")
    print("3. å»ºè®®å…ˆç”¨å°çš„ batch_size æµ‹è¯•ï¼Œç¡®ä¿ç¨³å®šåå†å¢åŠ ")
    print("4. ä½¿ç”¨ htop æˆ– nvidia-smi ç›‘æ§å®é™…å†…å­˜ä½¿ç”¨")
    print()
    
    # å†…å­˜ä¸è¶³è­¦å‘Š
    if available_memory < 4:
        print("âš ï¸  è­¦å‘Š: å¯ç”¨å†…å­˜ä¸è¶³4GBï¼Œå»ºè®®:")
        print("   - å…³é—­å…¶ä»–å ç”¨å†…å­˜çš„ç¨‹åº")
        print("   - ä½¿ç”¨ --batch-size 20 æˆ–æ›´å°")
        print("   - è€ƒè™‘åˆ†æ®µå¤„ç†æ•°æ®é›†")
        print()


def main():
    if len(sys.argv) > 1:
        try:
            total_frames = int(sys.argv[1])
        except ValueError:
            print("ç”¨æ³•: python diagnose_memory.py [æ€»å¸§æ•°]")
            print("ç¤ºä¾‹: python diagnose_memory.py 273465")
            sys.exit(1)
    else:
        total_frames = 273465  # é»˜è®¤å€¼
        print(f"æœªæŒ‡å®šæ€»å¸§æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼: {total_frames}")
        print()
    
    print_recommendations(total_frames)
    
    # äº¤äº’å¼å»ºè®®
    print("ğŸ¤” éœ€è¦æ›´å…·ä½“çš„å»ºè®®ï¼Ÿ")
    print("   è¿è¡Œ: python diagnose_memory.py <ä½ çš„æ•°æ®é›†æ€»å¸§æ•°>")
    print()


if __name__ == '__main__':
    main()

#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–çš„Pick/Placeæ•°æ®é›†è£å‰ªå’Œè½¬æ¢è„šæœ¬

æµç¨‹ï¼š
1. åŠ è½½LeRobotæ•°æ®é›†
2. æ£€æµ‹å¤¹çˆªçŠ¶æ€å˜åŒ–å…³é”®å¸§
3. æå–å‰åå„30å¸§
4. ä½¿ç”¨LLMç”Ÿæˆä»»åŠ¡æè¿°
5. è½¬æ¢ä¸ºLeRobotæ ¼å¼ä¿å­˜
"""

import torch
import numpy as np
import argparse
from pathlib import Path
import json
import sys
from typing import Optional
import time
from datetime import datetime

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from gripper_detector import analyze_gripper_changes
from task_description_generator import TaskDescriptionGenerator
from dataset_cutter import cut_and_convert_dataset


def load_lerobot_dataset(dataset_path: str = None):
    """
    åŠ è½½LeRobotæ•°æ®é›†
    """
    try:
        from lerobot.datasets.lerobot_dataset import LeRobotDataset
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…LeRobotåº“: pip install lerobot")
        sys.exit(1)
    
    if dataset_path is None:
        dataset_path = '/home/dongyingyibadao/HuggingFaceVLA_cus/libero'
    
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {dataset_path}")
    
    try:
        # ä½¿ç”¨ä¸data_dealerç›¸åŒçš„åŠ è½½æ–¹å¼
        dataset = LeRobotDataset(
            repo_id="HuggingFaceVLA_cus/libero",
            root=str(dataset_path)
        )
        print(f"âœ“ æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± {len(dataset)} å¸§")
        return dataset
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        sys.exit(1)


def analyze_and_extract(dataset,
                       start_idx: int = 0,
                       end_idx: int = 10000,
                       before_frames: int = 30,
                       after_frames: int = 30) -> tuple:
    """
    åˆ†ææ•°æ®é›†å¹¶æå–å…³é”®å¸§
    """
    print(f"\nğŸ” åˆ†ææ•°æ®é›† ({start_idx} - {end_idx})...")
    print(f"  - å…³é”®å¸§å‰: {before_frames} å¸§")
    print(f"  - å…³é”®å¸§å: {after_frames} å¸§")
    
    changes, frame_ranges = analyze_gripper_changes(
        dataset, 
        start_idx, 
        end_idx, 
        before_frames=before_frames,
        after_frames=after_frames,
        merge=False
    )
    
    return changes, frame_ranges


def generate_task_descriptions(frame_ranges: list,
                               dataset = None,
                               provider: str = 'local',
                               api_key: Optional[str] = None,
                               api_base: Optional[str] = None,
                               api_version: Optional[str] = None,
                               model: Optional[str] = None,
                               fast_mode: bool = False,
                               checkpoint_dir: Optional[Path] = None,
                               resume_from: Optional[str] = None) -> list:
    """
    ä¸ºå…³é”®å¸§ç”Ÿæˆä»»åŠ¡æè¿°ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    
    Args:
        checkpoint_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
        resume_from: ä»æ£€æŸ¥ç‚¹æ–‡ä»¶æ¢å¤
    """
    mode_str = "å¿«é€Ÿæ¨¡å¼(2å¸§)" if fast_mode else "ç²¾ç»†æ¨¡å¼(6å¸§)"
    print(f"\nğŸ¤– ç”Ÿæˆä»»åŠ¡æè¿°... [{mode_str}]")
    
    # å‡†å¤‡æ£€æŸ¥ç‚¹ç›®å½•
    if checkpoint_dir:
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹ä¿å­˜: {checkpoint_dir}")
    
    # å°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤
    start_idx = 0
    completed_ranges = []
    
    if resume_from and Path(resume_from).exists():
        print(f"\nğŸ“– ä»æ£€æŸ¥ç‚¹æ¢å¤: {resume_from}")
        try:
            with open(resume_from, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            
            completed_ranges = checkpoint_data.get('completed_ranges', [])
            start_idx = checkpoint_data.get('last_index', 0) + 1
            
            print(f"âœ“ å·²æ¢å¤ {len(completed_ranges)} ä¸ªå·²å®Œæˆçš„ä»»åŠ¡æè¿°")
            print(f"âœ“ ä»ç´¢å¼• {start_idx}/{len(frame_ranges)} ç»§ç»­å¤„ç†")
        except Exception as e:
            print(f"âš ï¸  è¯»å–æ£€æŸ¥ç‚¹å¤±è´¥: {e}ï¼Œä»å¤´å¼€å§‹")
            start_idx = 0
            completed_ranges = []
    
    kwargs = {'provider': provider}
    if api_key:
        kwargs['api_key'] = api_key
    if api_base:
        kwargs['api_base'] = api_base
    if api_version:
        kwargs['api_version'] = api_version
    if model:
        kwargs['model'] = model
    if provider.lower() == 'gpt':
        kwargs['fast_mode'] = fast_mode
    
    generator = TaskDescriptionGenerator(**kwargs)
    
    # å¸¦æ–­ç‚¹ä¿å­˜çš„æè¿°ç”Ÿæˆ
    ranges_with_desc = generator.generate_descriptions(
        frame_ranges, 
        dataset=dataset,
        start_index=start_idx,
        completed_ranges=completed_ranges,
        checkpoint_dir=checkpoint_dir
    )
    
    return ranges_with_desc


def save_frame_ranges_info(frame_ranges: list, output_path: Path):
    """
    ä¿å­˜å¸§èŒƒå›´ä¿¡æ¯ä¸ºJSON
    """
    def convert_to_serializable(val):
        """å°†ä»»ä½•å€¼è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        if isinstance(val, torch.Tensor):
            return int(val.item()) if val.numel() == 1 else val.tolist()
        elif isinstance(val, np.ndarray):
            return val.tolist()
        elif isinstance(val, (int, float, str, bool, type(None))):
            return val
        else:
            return str(val)
    
    info = {
        'total_ranges': len(frame_ranges),
        'frame_ranges': []
    }
    
    pick_count = 0
    place_count = 0
    
    for r in frame_ranges:
        if r['action_type'] == 'pick':
            pick_count += 1
        else:
            place_count += 1
        
        info['frame_ranges'].append({
            'id': len(info['frame_ranges']),
            'keyframe_index': convert_to_serializable(r['keyframe_index']),
            'action_type': r['action_type'],
            'frame_start': convert_to_serializable(r['frame_start']),
            'frame_end': convert_to_serializable(r['frame_end']),
            'num_frames': convert_to_serializable(r['num_frames']),
            'original_task': str(r['task']),
            'new_task': str(r.get('new_task', r['task'])),
            'episode_index': convert_to_serializable(r['episode_index']),
            'frame_index': convert_to_serializable(r['frame_index'])
        })
    
    info['pick_count'] = pick_count
    info['place_count'] = place_count
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(info, f, indent=2, ensure_ascii=False)
    
    print(f"âœ“ ä¿å­˜å¸§èŒƒå›´ä¿¡æ¯: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description='è‡ªåŠ¨åŒ–Pick/Placeæ•°æ®é›†è£å‰ªå’Œè½¬æ¢'
    )
    parser.add_argument('--dataset-path', type=str, default=None,
                       help='LeRobotæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--output-dir', type=str, 
                       default='./cut_dataset',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--start-idx', type=int, default=0,
                       help='å¼€å§‹ç´¢å¼•')
    parser.add_argument('--end-idx', type=int, default=None,
                       help='ç»“æŸç´¢å¼•ï¼ˆé»˜è®¤ï¼šå¤„ç†æ‰€æœ‰æ•°æ®ï¼‰')
    parser.add_argument('--max-episodes', type=int, default=None,
                       help='æœ€å¤šä¿å­˜çš„episodeæ•°é‡')
    parser.add_argument('--before-frames', type=int, default=30,
                       help='å…³é”®å¸§å‰å–çš„å¸§æ•°')
    parser.add_argument('--after-frames', type=int, default=30,
                       help='å…³é”®å¸§åå–çš„å¸§æ•°')
    parser.add_argument('--save-mode', type=str, default='lerobot',
                       choices=['image', 'lerobot', 'both'],
                       help='ä¿å­˜æ¨¡å¼: image(å›¾ç‰‡), lerobot(Parquet), both(ä¸¤è€…)')
    parser.add_argument('--llm-provider', type=str, default='local',
                       choices=['local', 'qwen', 'deepseek', 'gpt'],
                       help='LLMæä¾›è€…')
    parser.add_argument('--llm-api-key', type=str, default=None,
                       help='LLM APIå¯†é’¥')
    parser.add_argument('--llm-api-base', type=str, default=None,
                       help='LLM APIåŸºç¡€URL (ç”¨äºè‡ªå®šä¹‰/ä»£ç†æœåŠ¡)')
    parser.add_argument('--llm-api-version', type=str, default=None,
                       help='LLM APIç‰ˆæœ¬ (ç”¨äºAzure OpenAI)')
    parser.add_argument('--llm-model', type=str, default=None,
                       help='æŒ‡å®šLLMæ¨¡å‹åç§° (ä¾‹å¦‚: gpt-4o, gpt-4-turbo, o1-preview)')
    parser.add_argument('--llm-fast-mode', action='store_true',
                       help='GPTå¿«é€Ÿæ¨¡å¼ï¼šä»…ä¸Šä¼ 2å¸§å›¾åƒ(cam1é¦–å°¾å¸§)ï¼Œå¤„ç†é€Ÿåº¦æ›´å¿«')
    parser.add_argument('--checkpoint-interval', type=int, default=10,
                       help='æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”ï¼ˆæ¯å¤„ç†å¤šå°‘ä¸ªä¿å­˜ä¸€æ¬¡ï¼Œé»˜è®¤10ï¼‰')
    parser.add_argument('--resume-from', type=str, default=None,
                       help='ä»æ£€æŸ¥ç‚¹æ–‡ä»¶æ¢å¤ï¼ˆä¾‹å¦‚ï¼š./cut_dataset/checkpoints/checkpoint_latest.jsonï¼‰')
    parser.add_argument('--skip-cutting', action='store_true',
                       help='è·³è¿‡æ•°æ®é›†è£å‰ªï¼Œä»…ç”Ÿæˆåˆ†æ')
    parser.add_argument('--load-ranges', type=str, default=None,
                       help='åŠ è½½ä¹‹å‰ä¿å­˜çš„å¸§èŒƒå›´ä¿¡æ¯')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸš€ Pick/Place è‡ªåŠ¨åŒ–æ•°æ®é›†è£å‰ªå’Œè½¬æ¢")
    print("=" * 80)
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æˆ–ç”Ÿæˆå¸§èŒƒå›´ä¿¡æ¯
    ranges_info_file = output_dir / 'frame_ranges_info.json'
    
    if args.load_ranges:
        print(f"\nğŸ“– åŠ è½½ä¹‹å‰ä¿å­˜çš„å¸§èŒƒå›´ä¿¡æ¯: {args.load_ranges}")
        with open(args.load_ranges, 'r') as f:
            ranges_info = json.load(f)
        # é‡æ„frame_ranges
        frame_ranges = ranges_info['frame_ranges']
    else:
        # åŠ è½½æ•°æ®é›†
        dataset = load_lerobot_dataset(args.dataset_path)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®š end_idxï¼Œä½¿ç”¨æ•°æ®é›†æ€»é•¿åº¦
        end_idx = args.end_idx if args.end_idx is not None else len(dataset)
        
        print(f"ğŸ“Š å¤„ç†èŒƒå›´: {args.start_idx} - {end_idx} (å…± {end_idx - args.start_idx} å¸§)")
        if args.end_idx is None:
            print(f"   â„¹ï¸  æœªæŒ‡å®š --end-idxï¼Œå°†å¤„ç†æ‰€æœ‰æ•°æ®")
        
        # åˆ†æå’Œæå–
        changes, frame_ranges = analyze_and_extract(
            dataset, 
            args.start_idx, 
            end_idx,
            before_frames=args.before_frames,
            after_frames=args.after_frames
        )
        
        # ç”Ÿæˆä»»åŠ¡æè¿°
        checkpoint_dir = output_dir / 'checkpoints' if output_dir else None
        
        frame_ranges = generate_task_descriptions(
            frame_ranges,
            dataset=dataset,
            provider=args.llm_provider,
            api_key=args.llm_api_key,
            api_base=args.llm_api_base,
            api_version=args.llm_api_version,
            model=args.llm_model,
            fast_mode=args.llm_fast_mode,
            checkpoint_dir=checkpoint_dir,
            resume_from=args.resume_from
        )
        
        # ä¿å­˜å¸§èŒƒå›´ä¿¡æ¯
        save_frame_ranges_info(frame_ranges, ranges_info_file)
    
    # è£å‰ªæ•°æ®é›†
    if not args.skip_cutting:
        print(f"\nğŸ’¾ å¼€å§‹è£å‰ªå’Œè½¬æ¢æ•°æ®é›†...")
        print(f"ğŸ“¦ ä¿å­˜æ¨¡å¼: {args.save_mode}")
        
        # é‡æ–°åŠ è½½æ•°æ®é›†ï¼ˆå¦‚æœæ²¡æœ‰åŠ è½½çš„è¯ï¼‰
        if args.load_ranges:
            dataset = load_lerobot_dataset(args.dataset_path)
        
        output_path = cut_and_convert_dataset(
            dataset,
            frame_ranges,
            str(output_dir),
            save_mode=args.save_mode,
            max_episodes=args.max_episodes
        )
        
        print(f"\nâœ… æ•°æ®é›†è£å‰ªå’Œè½¬æ¢å®Œæˆ!")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_path}")
        
        if args.save_mode == 'image':
            print(f"ğŸ“‹ å›¾ç‰‡æ¨¡å¼: å¯ä»¥ç›´æ¥æŸ¥çœ‹ {output_path}/images/ ç›®å½•ä¸‹çš„å›¾ç‰‡")
        elif args.save_mode == 'lerobot':
            print(f"ğŸ“‹ LeRobotæ¨¡å¼: å¯ä»¥ä½¿ç”¨LeRobotDatasetåŠ è½½è®­ç»ƒ")
        else:
            print(f"ğŸ“‹ ä¸¤ç§æ¨¡å¼éƒ½å·²ä¿å­˜")
    else:
        print(f"\nâ­ï¸  å·²è·³è¿‡æ•°æ®é›†è£å‰ªæ­¥éª¤")
        print(f"ğŸ“‹ å¸§èŒƒå›´ä¿¡æ¯å·²ä¿å­˜: {ranges_info_file}")
    
    print("\n" + "=" * 80)


if __name__ == '__main__':
    main()

#!/usr/bin/env python3
"""
LeRobotæ•°æ®é›†è¯»å–å’Œæ£€æŸ¥è„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼Œæ— matplotlibä¾èµ–ï¼‰

ç”¨äºŽè¯»å–ã€æ£€æŸ¥è½¬æ¢åŽçš„LeRobotæ ‡å‡†æ ¼å¼æ•°æ®é›†
"""

import argparse
from pathlib import Path
import json
from typing import Optional
import sys

try:
    from lerobot.datasets.lerobot_dataset import LeRobotDataset
    import torch
    import numpy as np
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦çš„åº“: {e}")
    print("è¯·å®‰è£…: pip install lerobot torch")
    sys.exit(1)


def load_dataset(dataset_path: str, repo_id: Optional[str] = None):
    """
    åŠ è½½LeRobotæ•°æ®é›†
    
    Args:
        dataset_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
        repo_id: ä»“åº“IDï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨è·¯å¾„åï¼‰
    """
    print(f"ðŸ“‚ åŠ è½½æ•°æ®é›†: {dataset_path}")
    
    dataset_path = Path(dataset_path).resolve()
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not dataset_path.exists():
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
        sys.exit(1)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯LeRobotæ•°æ®é›†æ ¼å¼
    required_dirs = ['data', 'meta']
    if not all((dataset_path / d).exists() for d in required_dirs):
        print(f"âŒ ä¸æ˜¯æœ‰æ•ˆçš„LeRobotæ•°æ®é›†æ ¼å¼")
        print(f"   éœ€è¦åŒ…å«: {required_dirs}")
        sys.exit(1)
    
    # æ£€æŸ¥info.json
    info_file = dataset_path / 'meta' / 'info.json'
    if not info_file.exists():
        print(f"âš ï¸  ç¼ºå°‘ meta/info.json æ–‡ä»¶")
        print(f"   æ•°æ®é›†å¯èƒ½æ— æ³•è¢«LeRobotæ­£ç¡®åŠ è½½")
    
    try:
        # å¦‚æžœæ²¡æœ‰æŒ‡å®šrepo_idï¼Œä½¿ç”¨è·¯å¾„å
        if repo_id is None:
            repo_id = dataset_path.name
        
        print(f"ðŸ”§ åŠ è½½å‚æ•°:")
        print(f"   repo_id: {repo_id}")
        print(f"   root: {dataset_path}")
        
        # ç›´æŽ¥åŠ è½½æœ¬åœ°æ•°æ®é›†
        dataset = LeRobotDataset(
            repo_id=repo_id,
            root=str(dataset_path)
        )
        print(f"âœ“ æ•°æ®é›†åŠ è½½æˆåŠŸ")
        return dataset
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        print(f"\nè°ƒè¯•ä¿¡æ¯:")
        print(f"  - æ•°æ®é›†è·¯å¾„: {dataset_path}")
        print(f"  - repo_id: {repo_id}")
        if info_file.exists():
            print(f"  - info.json å­˜åœ¨")
            with open(info_file, 'r') as f:
                try:
                    info = json.load(f)
                    print(f"  - featuresæ•°: {len(info.get('features', {}))}")
                    print(f"  - data_path: {info.get('data_path', 'æœªè®¾ç½®')}")
                except:
                    print(f"  - info.json æ ¼å¼é”™è¯¯")
        print(f"\nç›®å½•ç»“æž„:")
        for item in sorted(dataset_path.iterdir())[:10]:
            print(f"  - {item.name}")
        sys.exit(1)


def print_dataset_info(dataset):
    """
    æ‰“å°æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
    """
    print("\n" + "=" * 80)
    print("ðŸ“Š æ•°æ®é›†åŸºæœ¬ä¿¡æ¯")
    print("=" * 80)
    
    print(f"æ€»å¸§æ•°: {len(dataset)}")
    
    # èŽ·å–å…ƒæ•°æ®
    if hasattr(dataset, 'meta'):
        meta = dataset.meta
        print(f"\nðŸ“‹ å…ƒæ•°æ®:")
        if hasattr(meta, 'fps'):
            print(f"  - FPS: {meta.fps}")
        if hasattr(meta, 'robot_type'):
            print(f"  - æœºå™¨äººç±»åž‹: {meta.robot_type}")
        if hasattr(meta, 'total_episodes'):
            print(f"  - Episodeæ•°é‡: {meta.total_episodes}")
        if hasattr(meta, 'total_frames'):
            print(f"  - æ€»å¸§æ•°: {meta.total_frames}")
    
    # æ£€æŸ¥ç¬¬ä¸€å¸§çš„æ•°æ®ç»“æž„
    print(f"\nðŸ” æ•°æ®ç»“æž„ï¼ˆç¬¬ä¸€å¸§ï¼‰:")
    first_frame = dataset[0]
    for key, value in first_frame.items():
        if isinstance(value, torch.Tensor):
            print(f"  - {key}: Tensor {value.shape} {value.dtype}")
        elif isinstance(value, np.ndarray):
            print(f"  - {key}: ndarray {value.shape} {value.dtype}")
        else:
            print(f"  - {key}: {type(value).__name__} = {value}")
    
    # ç»Ÿè®¡episodeä¿¡æ¯
    if 'episode_index' in first_frame:
        episode_indices = set()
        max_check = min(len(dataset), 10000)
        print(f"\nðŸ“¦ ç»Ÿè®¡Episodeä¿¡æ¯ï¼ˆæ£€æŸ¥å‰{max_check}å¸§ï¼‰...")
        for i in range(max_check):
            episode_indices.add(int(dataset[i]['episode_index']))
        print(f"  - æ£€æµ‹åˆ°çš„Episodeæ•°: {len(episode_indices)}")
        print(f"  - Episode ID: {sorted(episode_indices)}")


def print_episode_info(dataset, episode_idx: int = 0):
    """
    æ‰“å°æŒ‡å®šepisodeçš„è¯¦ç»†ä¿¡æ¯
    """
    print("\n" + "=" * 80)
    print(f"ðŸ“¦ Episode {episode_idx} è¯¦ç»†ä¿¡æ¯")
    print("=" * 80)
    
    # æ‰¾åˆ°è¯¥episodeçš„æ‰€æœ‰å¸§
    episode_frames = []
    for i in range(len(dataset)):
        frame = dataset[i]
        if int(frame['episode_index']) == episode_idx:
            episode_frames.append(i)
        if len(episode_frames) > 0 and int(frame['episode_index']) > episode_idx:
            break  # ä¼˜åŒ–ï¼šå·²ç»è¿‡äº†è¯¥episode
    
    if not episode_frames:
        print(f"âŒ æœªæ‰¾åˆ°Episode {episode_idx}")
        return
    
    print(f"æ€»å¸§æ•°: {len(episode_frames)}")
    print(f"å¸§ç´¢å¼•èŒƒå›´: {episode_frames[0]} - {episode_frames[-1]}")
    
    # æ£€æŸ¥ç¬¬ä¸€å¸§
    first_frame = dataset[episode_frames[0]]
    
    # æ‰“å°ä»»åŠ¡ä¿¡æ¯
    if 'task' in first_frame:
        print(f"\nðŸ“ ä»»åŠ¡ä¿¡æ¯:")
        print(f"  {first_frame['task']}")
    
    # æ£€æŸ¥å›¾åƒ
    image_keys = [k for k in first_frame.keys() if 'image' in k.lower() or 'cam' in k.lower()]
    if image_keys:
        print(f"\nðŸ“· å›¾åƒä¿¡æ¯:")
        for key in image_keys:
            img = first_frame[key]
            if isinstance(img, torch.Tensor):
                print(f"  - {key}: {img.shape} {img.dtype}")
    
    # æ£€æŸ¥åŠ¨ä½œ
    if 'action' in first_frame:
        action = first_frame['action']
        if isinstance(action, torch.Tensor):
            print(f"\nðŸŽ® åŠ¨ä½œä¿¡æ¯:")
            print(f"  - å½¢çŠ¶: {action.shape}")
            print(f"  - æ•°æ®ç±»åž‹: {action.dtype}")
            print(f"  - ç¬¬ä¸€å¸§åŠ¨ä½œ: {action.cpu().numpy()}")
            print(f"  - æœ€åŽå¸§åŠ¨ä½œ: {dataset[episode_frames[-1]]['action'].cpu().numpy()}")
    
    # æ£€æŸ¥çŠ¶æ€
    if 'state' in first_frame:
        state = first_frame['state']
        if isinstance(state, torch.Tensor):
            print(f"\nðŸ”§ çŠ¶æ€ä¿¡æ¯:")
            print(f"  - å½¢çŠ¶: {state.shape}")
            print(f"  - æ•°æ®ç±»åž‹: {state.dtype}")
            print(f"  - ç¬¬ä¸€å¸§çŠ¶æ€: {state.cpu().numpy()}")


def save_frame_image(dataset, frame_idx: int, output_path: str, camera_key: Optional[str] = None):
    """
    ä¿å­˜æŒ‡å®šå¸§çš„å›¾åƒåˆ°æ–‡ä»¶
    
    Args:
        dataset: LeRobotæ•°æ®é›†
        frame_idx: å¸§ç´¢å¼•
        output_path: è¾“å‡ºå›¾åƒè·¯å¾„
        camera_key: æŒ‡å®šç›¸æœºé”®ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªï¼‰
    """
    try:
        from PIL import Image
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…PIL: pip install pillow")
        return
    
    frame = dataset[frame_idx]
    
    # æ‰¾åˆ°å›¾åƒé”®
    image_keys = [k for k in frame.keys() if 'image' in k.lower() or 'cam' in k.lower()]
    
    if not image_keys:
        print("âŒ æœªæ‰¾åˆ°å›¾åƒæ•°æ®")
        return
    
    # é€‰æ‹©ç›¸æœº
    if camera_key is None:
        camera_key = image_keys[0]
    elif camera_key not in image_keys:
        print(f"âŒ æœªæ‰¾åˆ°ç›¸æœº {camera_key}ï¼Œå¯ç”¨: {image_keys}")
        return
    
    img = frame[camera_key]
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    if isinstance(img, torch.Tensor):
        img = img.cpu().numpy()
    
    # è°ƒæ•´ç»´åº¦é¡ºåº (C, H, W) -> (H, W, C)
    if img.shape[0] in [1, 3, 4]:
        img = np.transpose(img, (1, 2, 0))
    
    # å½’ä¸€åŒ–åˆ°0-255
    if img.max() <= 1.0:
        img = (img * 255).astype(np.uint8)
    
    # ä¿å­˜
    Image.fromarray(img).save(output_path)
    print(f"âœ“ å›¾åƒå·²ä¿å­˜: {output_path}")


def print_frame_range_info(dataset_path: str):
    """
    æ‰“å°frame_ranges_info.jsonçš„å†…å®¹
    """
    info_file = Path(dataset_path) / 'frame_ranges_info.json'
    
    if not info_file.exists():
        print(f"âš ï¸  æœªæ‰¾åˆ° frame_ranges_info.json")
        return
    
    print("\n" + "=" * 80)
    print("ðŸ“‹ å¸§èŒƒå›´ä¿¡æ¯ (frame_ranges_info.json)")
    print("=" * 80)
    
    with open(info_file, 'r', encoding='utf-8') as f:
        info = json.load(f)
    
    print(f"\næ€»ç‰‡æ®µæ•°: {info.get('total_ranges', 0)}")
    print(f"PickåŠ¨ä½œ: {info.get('pick_count', 0)}")
    print(f"PlaceåŠ¨ä½œ: {info.get('place_count', 0)}")
    
    print(f"\nè¯¦ç»†ä¿¡æ¯:")
    for r in info.get('frame_ranges', []):
        print(f"\n  [{r['id']}] {r['action_type'].upper()}")
        print(f"    å…³é”®å¸§: {r['keyframe_index']}")
        print(f"    å¸§èŒƒå›´: {r['frame_start']} - {r['frame_end']} ({r['num_frames']}å¸§)")
        print(f"    Episode: {r['episode_index']}, Frame: {r['frame_index']}")
        print(f"    åŽŸä»»åŠ¡: {r['original_task']}")
        print(f"    æ–°ä»»åŠ¡: {r['new_task']}")


def main():
    parser = argparse.ArgumentParser(
        description='LeRobotæ•°æ®é›†è¯»å–å’Œæ£€æŸ¥å·¥å…·ï¼ˆç®€åŒ–ç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬ä¿¡æ¯æŸ¥çœ‹
  python read_lerobot_dataset_simple.py --dataset-path ./cut_dataset
  
  # æŸ¥çœ‹ç‰¹å®šepisode
  python read_lerobot_dataset_simple.py --dataset-path ./cut_dataset --episode 0
  
  # ä¿å­˜æŒ‡å®šå¸§çš„å›¾åƒ
  python read_lerobot_dataset_simple.py --dataset-path ./cut_dataset --save-frame 0 --output frame_0.png
  
  # æŸ¥çœ‹å¸§èŒƒå›´ä¿¡æ¯
  python read_lerobot_dataset_simple.py --dataset-path ./cut_dataset --show-ranges
        """
    )
    
    parser.add_argument('--dataset-path', type=str, required=True,
                       help='LeRobotæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--repo-id', type=str, default=None,
                       help='ä»“åº“IDï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--episode', type=int, default=None,
                       help='æŸ¥çœ‹æŒ‡å®šepisodeçš„è¯¦ç»†ä¿¡æ¯')
    parser.add_argument('--save-frame', type=int, default=None,
                       help='ä¿å­˜æŒ‡å®šå¸§çš„å›¾åƒ')
    parser.add_argument('--output', type=str, default='frame.png',
                       help='å›¾åƒè¾“å‡ºè·¯å¾„ï¼ˆé…åˆ--save-frameä½¿ç”¨ï¼‰')
    parser.add_argument('--camera', type=str, default=None,
                       help='æŒ‡å®šç›¸æœºé”®ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--show-ranges', action='store_true',
                       help='æ˜¾ç¤ºframe_ranges_info.jsonå†…å®¹')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ðŸ“– LeRobot æ•°æ®é›†è¯»å–å·¥å…·")
    print("=" * 80)
    
    # æ˜¾ç¤ºå¸§èŒƒå›´ä¿¡æ¯
    if args.show_ranges:
        print_frame_range_info(args.dataset_path)
        print("\n" + "=" * 80)
        return
    
    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset(args.dataset_path, args.repo_id)
    
    # æ‰“å°åŸºæœ¬ä¿¡æ¯
    print_dataset_info(dataset)
    
    # æŸ¥çœ‹ç‰¹å®šepisode
    if args.episode is not None:
        print_episode_info(dataset, args.episode)
    
    # ä¿å­˜å¸§å›¾åƒ
    if args.save_frame is not None:
        print(f"\nðŸ’¾ ä¿å­˜å¸§ {args.save_frame}...")
        save_frame_image(dataset, args.save_frame, args.output, args.camera)
    
    print("\n" + "=" * 80)
    print("âœ… å®Œæˆ")
    print("=" * 80)


if __name__ == '__main__':
    main()
